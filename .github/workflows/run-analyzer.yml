name: AI Keyword Gap Analyzer

on:
  workflow_dispatch:
    inputs:
      seed_keyword:
        description: 'Từ khóa chính (VD: iphone 17)'
        required: true
        default: 'iphone 17'
      your_url:
        description: 'URL trang bạn (VD: https://cellphones.com.vn)'
        required: true
        default: 'https://cellphones.com.vn'

jobs:
  analyze:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests pandas beautifulsoup4 google-generativeai python-dotenv openpyxl

      - name: Create .env file
        run: |
          echo "GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }}" > .env
          echo "SERPAPI_KEY=${{ secrets.SERPAPI_KEY }}" >> .env

      - name: Run analyzer.py (NO INPUT - dùng GitHub input)
        env:
          SEED_KEYWORD: ${{ inputs.seed_keyword }}
          YOUR_URL: ${{ inputs.your_url }}
        run: |
          # Tạo file analyzer.py tạm với input bị thay thế
          cat > temp_analyzer.py << 'EOF'
          import os
          import requests
          import pandas as pd
          from bs4 import BeautifulSoup
          import google.generativeai as genai
          from dotenv import load_dotenv
          import json
          from datetime import datetime

          load_dotenv()
          genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
          SERPAPI_KEY = os.getenv("SERPAPI_KEY")

          if not os.getenv("GEMINI_API_KEY") or not SERPAPI_KEY:
              print("LỖI: Thiếu API key!")
              exit()

          model = genai.GenerativeModel('gemini-2.5-flash')
          print("AI sẵn sàng!")

          def get_serp_results(query, num=10):
              print(f"Đang tìm: {query}")
              params = {"q": query, "hl": "vi", "gl": "vn", "num": num, "api_key": SERPAPI_KEY}
              try:
                  response = requests.get("https://serpapi.com/search", params=params, timeout=30)
                  return response.json().get("organic_results", [])
              except Exception as e:
                  print(f"Lỗi kết nối SERP: {e}")
                  return []

          def extract_text_from_url(url):
              try:
                  headers = {'User-Agent': 'Mozilla/5.0'}
                  response = requests.get(url, headers=headers, timeout=15)
                  soup = BeautifulSoup(response.text, 'html.parser')
                  for tag in soup(["script", "style", "nav", "footer", "header"]):
                      tag.decompose()
                  text = soup.get_text(separator=' ')
                  return " ".join(text.split())[:4000]
              except:
                  return ""

          def ai_analyze(text, task):
              prompt = f"""
          Bạn là chuyên gia SEO. {task}

          Nội dung trang (giới hạn 3000 ký tự):
          {text[:3000]}

          Trả về đúng định dạng JSON, không giải thích:
          """
              try:
                  response = model.generate_content(prompt)
                  txt = response.text.replace("```json", "").replace("```", "").strip()
                  return json.loads(txt)
              except Exception as e:
                  print(f"   AI lỗi (bỏ qua): {e}")
                  return {}

          def analyze_seed_keyword(seed):
              print(f"\nPhân tích SERP cho: '{seed}'")
              results = get_serp_results(seed, 12)
              if not results:
                  return None

              keywords = []
              entities = []
              intents = []

              task = """
          Trích xuất từ nội dung trang:
          - top_keywords: 7 từ khóa chính (có thể là long-tail)
          - entities: 5 công cụ, khái niệm, thương hiệu
          - intent: informational / commercial / transactional / navigational
          """

              for i, res in enumerate(results[:7]):
                  url = res['link']
                  print(f"   Đọc {i+1}: {url[:50]}...")
                  text = extract_text_from_url(url)
                  if not text:
                      continue
                  data = ai_analyze(text, task)
                  keywords.extend([k.lower() for k in data.get("top_keywords", []) if len(k) > 2])
                  entities.extend(data.get("entities", []))
                  intents.append(data.get("intent", "unknown"))

              return {
                  "keywords": list(set(keywords)),
                  "entities": list(set(entities)),
                  "intent": max(set(intents), key=intents.count) if intents else "unknown"
              }

          def analyze_your_page(url):
              print(f"\nPhân tích trang bạn: {url}")
              text = extract_text_from_url(url)
              if not text:
                  print("   Không đọc được nội dung!")
                  return {"keywords": [], "entities": []}

              task = """
          Trích xuất từ nội dung trang:
          - keywords: danh sách tất cả từ khóa chính (tối thiểu 10)
          - entities: danh sách tất cả công cụ, khái niệm, thương hiệu
          """

              data = ai_analyze(text, task)
              return {
                  "keywords": [k.lower() for k in data.get("keywords", []) if len(k) > 2],
                  "entities": data.get("entities", [])
              }

          def find_gaps(serp_data, your_data):
              comp = set(serp_data["keywords"])
              your = set(your_data["keywords"])
              return {
                  "missing": sorted(list(comp - your), key=len, reverse=True)[:12],
                  "weak": list(your & comp)[:6]
              }

          def create_brief(keyword, serp_data):
              print(f"   Tạo brief: {keyword}")
              prompt = f"""
          Tạo content brief SEO cho từ khóa: "{keyword}"
          Intent: {serp_data['intent']}
          Entities: {', '.join(serp_data['entities'][:5])}
          Yêu cầu:
          - title
          - word_count: 1500-2500
          - h2: 6-8 tiêu đề
          - faq: 3 câu hỏi
          - schema: FAQSchema hoặc Article
          Trả về JSON.
          """
              try:
                  response = model.generate_content(prompt)
                  txt = response.text.replace("```json", "").replace("```", "").strip()
                  brief = json.loads(txt)
                  brief["keyword"] = keyword
                  return brief
              except Exception as e:
                  print(f"   Lỗi tạo brief: {e}")
                  return {
                      "keyword": keyword,
                      "title": f"{keyword} - Hướng Dẫn 2025",
                      "word_count": 1800,
                      "h2": [f"{keyword} là gì?", "Lợi ích", "Cách làm", "Công cụ", "Lưu ý", "Kết luận"],
                      "faq": [{"q": f"{keyword} là gì?", "a": "..."}],
                      "schema": "FAQSchema"
                  }

          def run_analysis():
              print("\n" + "="*60)
              print("     AI KEYWORD GAP ANALYZER - 2025")
              print("="*60)

              seed = "${{ env.SEED_KEYWORD }}"
              your_url = "${{ env.YOUR_URL }}"

              if not seed or not your_url.startswith("http"):
                  print("Dữ liệu không hợp lệ!")
                  return

              print(f"\nTừ khóa: {seed}")
              print(f"URL: {your_url}\n")

              serp_data = analyze_seed_keyword(seed)
              if not serp_data:
                  print("Lỗi SERP!")
                  return

              your_data = analyze_your_page(your_url)
              gaps = find_gaps(serp_data, your_data)

              print(f"\nKết quả: {len(gaps['missing'])} missing | {len(gaps['weak'])} weak")

              briefs = [create_brief(kw, serp_data) for kw in gaps["missing"][:3]]

              df = pd.DataFrame([
                  {"keyword": k, "type": "missing"} for k in gaps["missing"]
              ] + [
                  {"keyword": k, "type": "weak"} for k in gaps["weak"]
              ])

              report = {
                  "seed": seed,
                  "url": your_url,
                  "intent": serp_data["intent"],
                  "missing": len(gaps["missing"]),
                  "weak": len(gaps["weak"]),
                  "briefs": briefs,
                  "time": datetime.now().strftime("%Y-%m-%d %H:%M")
              }

              df.to_excel("GAP_REPORT.xlsx", index=False)
              with open("GAP_REPORT.json", "w", encoding="utf-8") as f:
                  json.dump(report, f, ensure_ascii=False, indent=2)

              print("\nHOÀN TẤT! File: GAP_REPORT.xlsx + GAP_REPORT.json")

          if __name__ == "__main__":
              run_analysis()
          EOF

          # Chạy file tạm
          python temp_analyzer.py

      - name: Upload reports
        uses: actions/upload-artifact@v4
        with:
          name: gap-reports
          path: |
            GAP_REPORT.xlsx
            GAP_REPORT.json

      - name: Commit reports
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add GAP_REPORT.xlsx GAP_REPORT.json || echo "No reports"
          git commit -m "Auto: Report for ${{ inputs.seed_keyword }}" || echo "Nothing to commit"
          git push || echo "No changes"
